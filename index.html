<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<!-- saved from url=(0042)https://research.nvidia.com/labs/par/calm/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252"><script src="" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

<style type="text/css">
body {
    font-family: "Titillium Web", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight: 300;
    font-size: 17px;
    margin-left: auto;
    margin-right: auto;
    width: 980px;
}
h1 {
    font-weight:300;
    line-height: 1.15em;
}

h2 {
    font-size: 1.75em;
}
a:link,a:visited {
    color: #1367a7;
    text-decoration: none;
}
a:hover {
    color: #208799;
}
h1, h2, h3 {
    text-align: center;
}
h1 {
    font-size: 40px;
    font-weight: 500;
}
h2 {
    font-weight: 400;
    margin: 16px 0px 4px 0px;
}
.paper-title {
    padding: 16px 0px 16px 0px;
}
section {
    margin: 32px 0px 32px 0px;
    text-align: justify;
    clear: both;
}
.col-6 {
     width: 16.6%;
     float: left;
}
.col-5 {
     width: 20%;
     float: left;
}
.col-4 {
     width: 25%;
     float: left;
}
.col-3 {
     width: 33%;
     float: left;
}
.col-2 {
     width: 50%;
     float: left;
}
.row, .author-row, .affil-row {
     overflow: auto;
}
.author-row, .affil-row {
    font-size: 20px;
}
.row {
    margin: 16px 0px 16px 0px;
}
.authors {
    font-size: 18px;
}
.affil-row {
    margin-top: 16px;
}
.teaser {
    max-width: 100%;
}
.text-center {
    text-align: center;  
}
.screenshot {
    width: 256px;
    border: 1px solid #ddd;
}
.screenshot-el {
    margin-bottom: 16px;
}
hr {
    height: 1px;
    border: 0; 
    border-top: 1px solid #ddd;
    margin: 0;
}
.material-icons {
    vertical-align: -6px;
}
p {
    line-height: 1.25em;
}
.caption {
    font-size: 16px;
    /*font-style: italic;*/
    color: #666;
    text-align: left;
    margin-top: 8px;
    margin-bottom: 8px;
}
video {
    display: block;
    margin: auto;
}
figure {
    display: block;
    margin: auto;
    margin-top: 10px;
    margin-bottom: 10px;
}
#bibtex pre {
    font-size: 14px;
    background-color: #eee;
    padding: 16px;
}
.blue {
    color: #2c82c9;
    font-weight: bold;
}
.orange {
    color: #d35400;
    font-weight: bold;
}
.flex-row {
    display: flex;
    flex-flow: row wrap;
    justify-content: space-around;
    padding: 0;
    margin: 0;
    list-style: none;
}
.paper-btn {
  position: relative;
  text-align: center;

  display: inline-block;
  margin: 8px;
  padding: 8px 8px;

  border-width: 0;
  outline: none;
  border-radius: 2px;
  
  background-color: #1367a7;
  color: #ecf0f1 !important;
  font-size: 20px;
  width: 100px;
  font-weight: 600;
}

.supp-btn {
  position: relative;
  text-align: center;

  display: inline-block;
  margin: 8px;
  padding: 8px 8px;

  border-width: 0;
  outline: none;
  border-radius: 2px;
  
  background-color: #1367a7;
  color: #ecf0f1 !important;
  font-size: 20px;
  width: 150px;
  font-weight: 600;
}

.paper-btn-parent {
    display: flex;
    justify-content: center;
    margin: 16px 0px;
}
.paper-btn:hover {
    opacity: 0.85;
}
.container {
    margin-left: auto;
    margin-right: auto;
    padding-left: 16px;
    padding-right: 16px;
}
.venue {
    color: #1367a7;
}

.kitti {
    display: flex;
    justify-content: center;
    margin: 16px 0px;
}

.topnav {
  overflow: hidden;
  background-color: #EEEEEE;
}

.topnav a {
  float: left;
  color: black;
  text-align: center;
  padding: 14px 16px;
  text-decoration: none;
  font-size: 16px;
}



</style>


</head><body><div class="topnav" id="myTopnav">
  <a href="https://www.nvidia.com/"><img width="100%" src="./files/nvidia.svg"></a>
  <a href="https://www.nvidia.com/en-us/research/"><strong>NVIDIA Research</strong></a>
</div>

<!-- End : Google Analytics Code -->
<script type="text/javascript" src="./files/style/hidebib.js"></script>
<link href="./files/style/css" rel="stylesheet" type="text/css">

    <title>Point-Cloud Completion with Pretrained Text-to-image Diffusion Models</title>
    <meta property="og:description" content="-Cloud Completion with Pretrained Text-to-image Diffusion Models">
    <link href="./files/style/css2" rel="stylesheet">

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async="" src="./files/style/js"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-6HHDEXF452');
</script>




 
<div class="container">
    <div class="paper-title">
      <h1>Point-Cloud Completion with Pretrained Text-to-image Diffusion Models</h1>
    </div>

    
    <div id="authors">
        <div class="author-row">
            <div class="col-3 text-center"><a href="https://research.nvidia.com/person/yoni-kasten">Yoni Kasten</a><sup>1</sup></div>
            <div class="col-3 text-center"><a href="">Ohad Rahamim</a><sup>2</sup></div>
            <div class="col-3 text-center"><a href="https://research.nvidia.com/person/gal-chechik">Gal Chechik</a><sup>1,2</sup></div>
        </div>

        <div class="affil-row">
            <div class="venue text-center"><sup>1</sup>NVIDIA</div>
        </div>
        <div class="affil-row">
            <div class="venue text-center"><sup>2</sup>Bar-Ilan University</div>
        </div>
        <div class="affil-row">
            <div class="venue text-center"><b>Arxiv 2023</b></div>
        </div>

        <div style="clear: both">
            <div class="paper-btn-parent">
            <a class="supp-btn" href="">
                <span class="material-icons">  </span> 
                 Paper
            </a>
            <a class="supp-btn" href="">
                <span class="material-icons">  </span> 
                  Code </br>
                  (Coming Soon)
            </a>
        </div></div>
    </div>

    <section id="teaser">
            <figure style="width: 100%;">
                <a href="./files/teaser.png">
                    <img width="100%" src="./files/teaser.png">
                </a>
                <p class="caption" style="margin-bottom: 1px;">
                    We present SDS-Complete: A test-time optimization method for completing point clouds, captured by depth sensors, into complete surface representation using pre-trained text-to-image diffusion model. Our method takes two inputs: an incomplete point cloud (blue) and a textual description of the object ("A chair"). It  outputs a complete surface (gray) that is consistent with the input points (blue).
                </p>
            </figure>
    </section>

    <section id="abstract">
        <h2>Abstract</h2>
        <hr>
        <p>
            Point-cloud data collected in real-world applications are often incomplete. Data is typically missing due to objects being observed from partial viewpoints, which only capture a specific perspective or angle. Additionally, data can be incomplete due to occlusion and low-resolution sampling.
Existing completion approaches rely on datasets of  predefined objects to guide the completion of noisy and incomplete, point clouds. However, these approaches perform poorly when tested on 
Out-Of-Distribution (OOD) objects, that are poorly represented in the training dataset.
Here we leverage  recent advances in text-guided image generation, which lead to major breakthroughs in text-guided shape generation. 
We describe an approach called SDS-complete that 
uses a pre-trained text-to-image diffusion model and leverages the text semantics of a given incomplete point cloud of an object, to obtain a complete surface representation.
SDS-complete can complete a variety of objects using test-time optimization without expensive collection of 3D information. We evaluate SDS-complete on incomplete scanned objects, captured by real-world depth sensors and LiDAR scanners. We find that it effectively reconstructs objects that are absent from common datasets, reducing Chamfer loss by 50% on average compared with current methods.  

        </p>
    </section>

    <hr>
    <h1>Overview</h1>
    <hr>

    <section id="teaser-videos">
        <div class="flex-row">
        <figure style="width: 100%;">
            <video width="90%" controls="" muted="" loop="" autoplay="">
                <source src="./files/00034.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </figure>
            <div style="width: 100%;">
                <br><br>
                <p>
                  In this work, we address this challenge of OOD objects by leveraging a pretrained text-to-image diffusion model.
                  It has been shown that these models, even though never trained on 3D data, can be used for text-guided 3D shape generation.
                  This is done through the SDS loss, which measures the agreement of the 3D shape's rendered images with the model prior. 
                  Our key idea is that since text-to-image diffusion models were trained on a vast number of diverse objects, they contain a strong prior about the shape and texture of objects, and that prior can be used for completing missing parts. For example, given a partial point cloud, knowing that it corresponds to a chair can guide the completion process, because objects from this class are expected to exhibit some types of symmetries and parts. 
                </p> 

                <p>
The key challenge in this approach is to combine the prior information from the diffusion model with the observed partial point cloud, to generate a complete shape that is faithful to the partial observations. We introduce SDS-complete: a point cloud completion method that uses the SDS-loss to accurately complete object surfaces while being guided by input constraints of text and point clouds.  To be consistent with the input points, we use a Signed Distance Function (SDF) surface representation, and constrain the zero level set of the SDF to go through the input points. SDS-complete enables overcoming the limitations of working with OOD objects as it brings the semantics from a pretrained text-to-image diffusion model. That allows us to generate accurate and realistic 3D shapes from partial observations.
                </p>
                
                                <p>
We demonstrate that SDS-complete generates completions for various objects with different shape types from two real-world datasets: the Redwood dataset, which contains various incomplete real-world depth camera scans, and the KITTI dataset, which contains object LiDAR scans from driving scenarios. In both cases, we outperform state-of-the-art methods for OOD objects, while showing comparable results on object classes that were used to train these methods. 
                </p>     
            </div>
        </div>
    </section>

    <section id="pipeline">

        <h2>Pipeline</h2>
        <hr>

        <figure style="width: 100%;">
            <video width="90%" controls="" muted="" loop="" autoplay="">
                <source src="./files/for_video.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <p class="caption" style="margin-bottom: 1px;">
                <div class="mathjax">
    <p>The components of our SDS-Complete approach. Our method optimizes two neural functions: A signed distance function \(f_\theta\) representing the surface and a volumetric coloring function \(\vec{c}_\varphi\). Together, \((\vec{c}_\varphi,f_\theta)\) define a radiance field, which is used to render novel image views \(Im_0,\ldots Im_n\). The SDS-Loss is applied to these renderings and encourages them to be compatible with the input text \(\vec{y}\). To constrain the surface to lie on the input points, we encourage the signed distance function to be zero at the input points (Sensor compatibility loss).</p>
</div>
            </p>
        </figure>

        <h2>Results</h2>
        <hr>
        We conduct an evaluation of our model, on two real-world datasets that encompass a diverse array of general objects. Our primary objective in selecting these datasets is to demonstrate that our proposed method can handle diverse variations of object types that are not confined to any specific domain. 

        <h2>Redwood dataset</h2>
        <hr>
        
        <section id="teaser">
            <figure style="width: 100%;">
                <a href="./Example_titles.png">
                    <img width="100%" src="./files/Example_titles.png">
                </a>
            </figure>
        </section>

        <figure style="display: inline; width: 33%;">
            <video style="display: inline;" width="49.5%" controls="" muted="" loop="" autoplay="">
                <source src="./files/01184.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </figure>
        <figure style="display: inline; width: 49%;">
            <video style="display: inline;" width="49.5%" controls="" muted="" loop="" autoplay="">
                <source src="./files/04797.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </figure>
        <figure style="display: inline; width: 33%;">
            <video style="display: inline;" width="49.5%" controls="" muted="" loop="" autoplay="">
                <source src="./files/09484.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </figure>
        

        <figure style="display: inline; width: 33%;">
            <video style="display: inline;" width="49.5%" controls="" muted="" loop="" autoplay="">
                <source src="./files/06127.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </figure>
        <figure style="display: inline; width: 33%;">
            <video style="display: inline;" width="49.5%" controls="" muted="" loop="" autoplay="">
                <source src="./files/06145.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </figure>
        

        <figure style="display: inline; width: 33%;">
            <video style="display: inline;" width="49.5%" controls="" muted="" loop="" autoplay="">
                <source src="./files/07306.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </figure>
        <figure style="display: inline; width: 33%;">
            <video style="display: inline;" width="49.5%" controls="" muted="" loop="" autoplay="">
                <source src="./files/09639.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </figure>
        
        <figure style="display: inline; width: 33%;">
            <video style="display: inline;" width="49.5%" controls="" muted="" loop="" autoplay="">
                <source src="./files/09424.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </figure>
        <figure style="display: inline; width: 33%;">
            <video style="display: inline;" width="49.5%" controls="" muted="" loop="" autoplay="">
                <source src="./files/04154.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </figure>
        <figure style="display: inline; width: 33%;">
            <video style="display: inline;" width="49.5%" controls="" muted="" loop="" autoplay="">
                <source src="./files/04457.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            
            <p class="caption" style="margin-bottom: 1px;">
            Example arranged in two columns, where each column has the following structure: 
                Left: The partial scans that are given as input to our model. Middle: the completed surface. Right: the completed surface together with the input points.
                 For more examples, please refer to the paper.
            </p>
        </figure>
        
        
        <h2>KITTI dataset</h2>
        <hr>
        
        <figure style="display: inline; width: 33%;">
            <video style="display: inline;" width="24%" controls="" muted="" loop="" autoplay="">
                <source src="./files/KITTI/motorcycle5_ours.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </figure>
        <figure style="display: inline; width: 33%;">
            <video style="display: inline;" width="24%" controls="" muted="" loop="" autoplay="">
                <source src="./files/KITTI/car_ours.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </figure>
        <figure style="display: inline; width: 33%;">
            <video style="display: inline;" width="24%" controls="" muted="" loop="" autoplay="">
                <source src="./files/KITTI/motorcycle7_ours.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </figure>

        <figure style="display: inline; width: 33%;">
            <video style="display: inline;" width="24%" controls="" muted="" loop="" autoplay="">
                <source src="./files/KITTI/car2_ours.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </figure>
        <figure style="display: inline; width: 33%;">
            <video style="display: inline;" width="24%" controls="" muted="" loop="" autoplay="">
                <source src="./files/KITTI/motorcycle10_ours.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </figure>
        <figure style="display: inline; width: 33%;">
            <video style="display: inline;" width="24%" controls="" muted="" loop="" autoplay="">
                <source src="./files/KITTI/truck_ours.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </figure>
        
        <figure style="display: inline; width: 33%;">
            <video style="display: inline;" width="24%" controls="" muted="" loop="" autoplay="">
                <source src="./files/KITTI/truck7_ours.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </figure>
        <figure style="display: inline; width: 33%;">
            <video style="display: inline;" width="24%" controls="" muted="" loop="" autoplay="">
                <source src="./files/KITTI/truck13_ours.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            
                        <p class="caption" style="margin-bottom: 1px;">
                The completed surface (gray) together with the input points (blue).
            </p>
        </figure>

    </section>
    
    <section id="bibtex">
        <h2>Citation</h2>
        <pre><code>

        </code></pre>
    </section>

    

<br>


</div>


</body></html>