<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<!-- saved from url=(0042)https://research.nvidia.com/labs/par/calm/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252"><script src="" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

<style type="text/css">
body {
    font-family: "Titillium Web", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight: 300;
    font-size: 17px;
    margin-left: auto;
    margin-right: auto;
    width: 980px;
}
h1 {
    font-weight:300;
    line-height: 1.15em;
}

h2 {
    font-size: 1.75em;
}
a:link,a:visited {
    color: #1367a7;
    text-decoration: none;
}
a:hover {
    color: #208799;
}
h1, h2, h3 {
    text-align: center;
}
h1 {
    font-size: 40px;
    font-weight: 500;
}
h2 {
    font-weight: 400;
    margin: 16px 0px 4px 0px;
}
.paper-title {
    padding: 16px 0px 16px 0px;
}
section {
    margin: 32px 0px 32px 0px;
    text-align: justify;
    clear: both;
}
.col-6 {
     width: 16.6%;
     float: left;
}
.col-5 {
     width: 20%;
     float: left;
}
.col-4 {
     width: 25%;
     float: left;
}
.col-3 {
     width: 33%;
     float: left;
}
.col-2 {
     width: 50%;
     float: left;
}
.row, .author-row, .affil-row {
     overflow: auto;
}
.author-row, .affil-row {
    font-size: 20px;
}
.row {
    margin: 16px 0px 16px 0px;
}
.authors {
    font-size: 18px;
}
.affil-row {
    margin-top: 16px;
}
.teaser {
    max-width: 100%;
}
.text-center {
    text-align: center;  
}
.screenshot {
    width: 256px;
    border: 1px solid #ddd;
}
.screenshot-el {
    margin-bottom: 16px;
}
hr {
    height: 1px;
    border: 0; 
    border-top: 1px solid #ddd;
    margin: 0;
}
.material-icons {
    vertical-align: -6px;
}
p {
    line-height: 1.25em;
}
.caption {
    font-size: 16px;
    /*font-style: italic;*/
    color: #666;
    text-align: left;
    margin-top: 8px;
    margin-bottom: 8px;
}
video {
    display: block;
    margin: auto;
}
figure {
    display: block;
    margin: auto;
    margin-top: 10px;
    margin-bottom: 10px;
}
#bibtex pre {
    font-size: 14px;
    background-color: #eee;
    padding: 16px;
}
.blue {
    color: #2c82c9;
    font-weight: bold;
}
.orange {
    color: #d35400;
    font-weight: bold;
}
.flex-row {
    display: flex;
    flex-flow: row wrap;
    justify-content: space-around;
    padding: 0;
    margin: 0;
    list-style: none;
}
.paper-btn {
  position: relative;
  text-align: center;

  display: inline-block;
  margin: 8px;
  padding: 8px 8px;

  border-width: 0;
  outline: none;
  border-radius: 2px;
  
  background-color: #1367a7;
  color: #ecf0f1 !important;
  font-size: 20px;
  width: 100px;
  font-weight: 600;
}

.supp-btn {
  position: relative;
  text-align: center;

  display: inline-block;
  margin: 8px;
  padding: 8px 8px;

  border-width: 0;
  outline: none;
  border-radius: 2px;
  
  background-color: #1367a7;
  color: #ecf0f1 !important;
  font-size: 20px;
  width: 150px;
  font-weight: 600;
}

.paper-btn-parent {
    display: flex;
    justify-content: center;
    margin: 16px 0px;
}
.paper-btn:hover {
    opacity: 0.85;
}
.container {
    margin-left: auto;
    margin-right: auto;
    padding-left: 16px;
    padding-right: 16px;
}
.venue {
    color: #1367a7;
}

.kitti {
    display: flex;
    justify-content: center;
    margin: 16px 0px;
}

.topnav {
  overflow: hidden;
  background-color: #EEEEEE;
}

.topnav a {
  float: left;
  color: black;
  text-align: center;
  padding: 14px 16px;
  text-decoration: none;
  font-size: 16px;
}



</style>


</head><body><div class="topnav" id="myTopnav">
  <a href="https://www.nvidia.com/"><img width="100%" src="./files/nvidia.svg"></a>
  <a href="https://www.nvidia.com/en-us/research/"><strong>NVIDIA Research</strong></a>
</div>

<!-- End : Google Analytics Code -->
<script type="text/javascript" src="./files/style/hidebib.js"></script>
<link href="./files/style/css" rel="stylesheet" type="text/css">

    <title>Point-Cloud Completion with Pretrained Text-to-image Diffusion Models</title>
    <meta property="og:description" content="-Cloud Completion with Pretrained Text-to-image Diffusion Models">
    <link href="./files/style/css2" rel="stylesheet">

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async="" src="./files/style/js"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-6HHDEXF452');
</script>




 
<div class="container">
    <div class="paper-title">
      <h1>Point-Cloud Completion with Pretrained Text-to-image Diffusion Models</h1>
    </div>

    
    <div id="authors">
        <div class="author-row">
            <div class="col-3 text-center"><a href="https://ykasten.github.io/">Yoni Kasten</a><sup>1</sup></div>
            <div class="col-3 text-center"><a href="https://ohad204.github.io/ohadrahamim.github.io/">Ohad Rahamim</a><sup>2</sup></div>
            <div class="col-3 text-center"><a href="https://research.nvidia.com/person/gal-chechik">Gal Chechik</a><sup>1,2</sup></div>
        </div>

        <div class="affil-row">
            <div class="venue text-center"><sup>1</sup>NVIDIA</div>
        </div>
        <div class="affil-row">
            <div class="venue text-center"><sup>2</sup>Bar-Ilan University</div>
        </div>
        <div class="affil-row">
            <div class="venue text-center"><b>NeurIPS 2023</b></div>
        </div>

        <div style="clear: both">
            <div class="paper-btn-parent">
            <a class="supp-btn" href="https://arxiv.org/pdf/2306.10533.pdf">
                <span class="material-icons">  </span> 
                 Paper
            </a>
            <a class="supp-btn" href="https://github.com/NVlabs/sds-complete">
                <span class="material-icons">  </span> 
                  Data 
            </a>
        </div></div>
    </div>

    <section id="teaser">
            <figure style="width: 100%;">
                <a href="./files/teaser.png">
                    <img width="100%" src="./files/teaser.png">
                </a>
                <p class="caption" style="margin-bottom: 1px;">
                    We present SDS-Complete: A test-time optimization method for completing point clouds captured by depth sensors, leveraging pre-trained text-to-image diffusion model. The inputs to our method are an incomplete point cloud (blue) along with a textual description of the object. The output is a complete surface (gray) that is consistent with the input points (blue). The method works well on a variety of objects captured by real-world point-cloud sensors.
                </p>
            </figure>
    </section>

    <section id="abstract">
        <h2>Abstract</h2>
        <hr>
        <p>
            Point-cloud data collected in real-world applications are often incomplete, because objects are being observed from specific viewpoints, which only capture one perspective. Data can also be incomplete due to occlusion and low-resolution sampling. Existing approaches to completion rely on training models with datasets of predefined objects to guide the completion of point clouds. Unfortunately, these approaches fail to generalize when tested on objects or real-world setups that are poorly represented in their training set. Here, we leverage recent advances in text-guided 3D shape generation, showing how to use image priors for generating 3D objects. We describe an approach called SDS-Complete that uses a pre-trained text-to-image diffusion model and leverages the text semantics of a given incomplete point cloud of an object, to obtain a complete surface representation. SDS-Complete can complete a variety of objects using test-time optimization without expensive collection of 3D data. We evaluate SDS-Complete on a collection of incomplete scanned objects, captured by real-world depth sensors and LiDAR scanners. We find that it effectively reconstructs objects that are absent from common datasets, reducing Chamfer loss by about 50% on average compared with current methods.
        </p>
    </section>

    <hr>
    <h1>Overview</h1>
    <hr>

    <section id="teaser-videos">
        <div class="flex-row">
        <figure style="width: 100%;">
            <video width="90%" controls="" muted="" loop="" autoplay="">
                <source src="./files/00034.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </figure>
            <div style="width: 100%;">
                <br><br>
                <p>
                    Here, we address the challenge of completing 3D objects in the wild from real-world partial point
                    clouds. This is achieved by leveraging priors about object shapes that are encoded in pretrained
                    text-to-image diffusion models. Our key idea is that since text-to-image diffusion models were trained
                    on a vast number of diverse objects, they contain a strong prior about the shape and texture of objects,
                    and that prior can be used for completing object missing parts. For example, given a partial point
                    cloud, knowing that it corresponds to a chair can guide the completion process, because objects from
                    this class are expected to exhibit some types of symmetries and parts that are captured in 2D images.
                </p> 

                <p>
                    A similar intuition has been used for generating 3D objects â€œfrom scratch" (DreamFusion).
                    DreamFusion uses the SDS loss, which measures agreement between 2D model prior and renderings
                    of the 3D shape. Unfortunately, naively applying the SDS loss to our problem of point cloud
                    completion fails. This is because, as we show below, it does not combine well the hard constraints
                    implied by the points collected from the sensor with the prior embedded in the diffusion model.
                </p>
                
                                <p>
                                    To address these challenges, we introduce SDS-Complete: a method to complete a given partial
point cloud using several considerations. First, we use a Signed Distance Function (SDF) surface
representation, and constrain the zero level set of the SDF to go through the input
points. Second, we use information about areas with no collected points, to rule out object parts
in these areas. Third, we use a prior about camera position and orientation and a curriculum of
out-painting when sampling camera positions. Finally, we use the SDS loss to incorporate prior
guided by the class of an object on the rendered images.
                </p>   
                
                <p>
                    We demonstrate that SDS-Complete generates completions for various objects with different shape
types from two real-world datasets: the Redwood dataset, which contains various incomplete
real-world depth camera partial scans, and the KITTI dataset, which contains object LiDAR
scans from driving scenarios. In both cases, SDS-Complete outperforms the current state-of-the-art
methods.
                </p>
            </div>
        </div>
    </section>

    <section id="pipeline">

        <h2>Pipeline</h2>
        <hr>

        <figure style="width: 100%;">
            <video width="90%" controls="" muted="" loop="" autoplay="">
                <source src="./files/SDS-Complete_pipeline_video.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <p class="caption" style="margin-bottom: 1px;">
                <div class="mathjax">
    

    SDS-loss optimizes two neural functions: A signed distance function \(f_\theta\) representing the surface and a volumetric coloring function \(\mathbf{c}_\varphi\). Together, \((\mathbf{c}_\varphi,f_\theta)\) define a radiance field, which is used to render novel image views \(Im_0,\ldots Im_n\). The SDS loss is applied to the renderings to encourage them to be compatible with the input text \(\mathbf{y}\). Three sensor-compatibility losses verify that the reconstructed surface is compatible with the sensor observations in various aspects. </p>
</div>
            </p>
        </figure>

        <h2>Results</h2>
        <hr>
        Our primary goal is to evaluate our SDS-Complete and baseline method in real-world scenarios. This is in contrast to evaluating test splits from the synthetic datasets that were used for training the baseline methods. To achieve relevant evaluation datasets, we based the evaluation on partial real-world point clouds obtained from depth images and LiDAR scans.
        <h2>Redwood dataset</h2>
        <hr>
        
        <section id="teaser">
            <figure style="width: 100%;">
                <a href="./Example_titles.png">
                    <img width="100%" src="./files/Example_titles.png">
                </a>
            </figure>
        </section>

        <figure style="display: inline; width: 33%;">
            <video style="display: inline;" width="49.5%" controls="" muted="" loop="" autoplay="">
                <source src="./files/01184.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </figure>
        <figure style="display: inline; width: 49%;">
            <video style="display: inline;" width="49.5%" controls="" muted="" loop="" autoplay="">
                <source src="./files/04797.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </figure>
        <figure style="display: inline; width: 33%;">
            <video style="display: inline;" width="49.5%" controls="" muted="" loop="" autoplay="">
                <source src="./files/09484.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </figure>
        

        <figure style="display: inline; width: 33%;">
            <video style="display: inline;" width="49.5%" controls="" muted="" loop="" autoplay="">
                <source src="./files/06127.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </figure>
        <figure style="display: inline; width: 33%;">
            <video style="display: inline;" width="49.5%" controls="" muted="" loop="" autoplay="">
                <source src="./files/06145.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </figure>
        

        <figure style="display: inline; width: 33%;">
            <video style="display: inline;" width="49.5%" controls="" muted="" loop="" autoplay="">
                <source src="./files/07306.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </figure>
        <figure style="display: inline; width: 33%;">
            <video style="display: inline;" width="49.5%" controls="" muted="" loop="" autoplay="">
                <source src="./files/09639.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </figure>
        
        <figure style="display: inline; width: 33%;">
            <video style="display: inline;" width="49.5%" controls="" muted="" loop="" autoplay="">
                <source src="./files/09424.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </figure>
        <figure style="display: inline; width: 33%;">
            <video style="display: inline;" width="49.5%" controls="" muted="" loop="" autoplay="">
                <source src="./files/04154.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </figure>
        <figure style="display: inline; width: 33%;">
            <video style="display: inline;" width="49.5%" controls="" muted="" loop="" autoplay="">
                <source src="./files/04457.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            
            <p class="caption" style="margin-bottom: 1px;">
            Example arranged in two columns, where each column has the following structure: 
                Left: The partial scans that are given as input to our model. Middle: the completed surface. Right: the completed surface together with the input points.
                 For more examples, please refer to the paper.
            </p>
        </figure>
        
        
        <h2>KITTI dataset</h2>
        <hr>
        
        <figure style="display: inline; width: 33%;">
            <video style="display: inline;" width="24%" controls="" muted="" loop="" autoplay="">
                <source src="./files/KITTI/motorcycle5_ours.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </figure>
        <figure style="display: inline; width: 33%;">
            <video style="display: inline;" width="24%" controls="" muted="" loop="" autoplay="">
                <source src="./files/KITTI/car_ours.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </figure>
        <figure style="display: inline; width: 33%;">
            <video style="display: inline;" width="24%" controls="" muted="" loop="" autoplay="">
                <source src="./files/KITTI/motorcycle7_ours.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </figure>

        <figure style="display: inline; width: 33%;">
            <video style="display: inline;" width="24%" controls="" muted="" loop="" autoplay="">
                <source src="./files/KITTI/car2_ours.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </figure>
        <figure style="display: inline; width: 33%;">
            <video style="display: inline;" width="24%" controls="" muted="" loop="" autoplay="">
                <source src="./files/KITTI/motorcycle10_ours.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </figure>
        <figure style="display: inline; width: 33%;">
            <video style="display: inline;" width="24%" controls="" muted="" loop="" autoplay="">
                <source src="./files/KITTI/truck_ours.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </figure>
        
        <figure style="display: inline; width: 33%;">
            <video style="display: inline;" width="24%" controls="" muted="" loop="" autoplay="">
                <source src="./files/KITTI/truck7_ours.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </figure>
        <figure style="display: inline; width: 33%;">
            <video style="display: inline;" width="24%" controls="" muted="" loop="" autoplay="">
                <source src="./files/KITTI/truck13_ours.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            
                        <p class="caption" style="margin-bottom: 1px;">
                The completed surface (gray) together with the input points (blue).
            </p>
        </figure>

    </section>
    
    <section id="bibtex">
        <h2>Citation</h2>
        <pre><code>
@article{kasten2023point,
  title={Point-Cloud Completion with Pretrained Text-to-image Diffusion Models},
  author={Kasten, Yoni and Rahamim, Ohad and Chechik, Gal},
  journal={arXiv preprint arXiv:2306.10533},
  year={2023}
}
        </code></pre>
    </section>

    

<br>


</div>


</body></html>
